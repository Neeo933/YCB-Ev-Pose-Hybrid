%!TEX encoding = UTF-8 Unicode

%---------------------------------------------------------------%
%  使用XeLaTex编译
%  参考文献的排版，请创建 .bib 文件， 并使用 BibTex 或者 Biber(有中文参考文献时) 进行排版。
%---------------------------------------------------------------%

%===============================================================%
%  固定模板，请不要修改这一部分

\documentclass[a4paper,11pt,onecolumn,UTF8]{article}
\usepackage{CVClassTemplate}  
\usepackage{booktabs}  % 用于 \toprule, \midrule, \bottomrule
\usepackage{array}     % 表格格式支持                                
\setmainfont{Times New Roman}                      

\newcommand{\mysecondauthor}{null}
\newcommand{\mythirdauthor}{null} 
\newcommand{\myfourthauthor}{null}
\newcommand{\myfifthauthor}{null}
%===============================================================%


%===============================================================%
% ** 请从这里开始你的编辑 **

%---------------------------------------------------------------%
%  基本信息配置
%---------------------------------------------------------------%
\newcommand{\mytitle}          % 请输入论文题目
{面向复杂环境与高速运动场景的事件驱动6D物体姿态估计}

\newcommand{\myfirstauthor}        % 请输入作者姓名
{\kaishu 南晓璐}

\newcommand{\myfirstaffiliation}   % 请输入作者单位
{\small 清华大学航天航空学院}

\newcommand{\myfirstemail}         % 请输入作者电子邮箱
{\small nxl25@mails.tsinghua.edu.cn}

\renewcommand{\mysecondauthor}{\kaishu 许嘉杰}    % 若有两名作者取消该行注释
\newcommand{\mysecondaffiliation}{\small 清华大学航天航空学院}
\newcommand{\mysecondemail}{\small xujiajie25@mails.tsinghua.edu.cn}

%\renewcommand{\mythirdauthor}{\kaishu 第三作者}    % 若有三名作者取消该行注释
\newcommand{\mythirdaffiliation}{\small 单位3}
\newcommand{\mythirdemail}{\small xxxx@xxxx.xxx}

%---------------------------------------------------------------%
%  摘要 & 关键词
%---------------------------------------------------------------%
\newcommand{\myabstract}       % 请在下面输入中文摘要
{
    在太空非合作目标抓取、高速工业流水线等极端视觉场景中，传统视觉方法面临巨大挑战。RGB相机在强光直射下易过曝，在高速相对运动下产生运动模糊，导致6D姿态估计失效作为神经形态传感器，事件相机异步记录亮度变化，并将这些变化以稀疏事件流的形式呈现，具有高时间分辨率和高动态范围的优势。为了解决这一问题，本文提出了一种融合事件相机数据的几何引导多模态网络 —— \textbf{GMG-PVNet}。

    该方法包含三个核心创新：(1) 引入 \textbf{MTS (Motion-Texture Surface)} 表征，将高频事件流转化为纹理特征，捕捉不受光照影响的运动边缘；(2) 设计 \textbf{AFDM (Attentive Feature Diffusion Module)}，利用稀疏的事件点云引导密集图像特征的增强，解决了稀疏-密集模态融合的难题；(3) 提出 \textbf{Gated Fusion} 机制，自适应地融合纹理与几何特征。实验结果表明，该方法在YCB-Video数据集的单物体模型上实现了超过90\%的ADD-S精度，在多物体复杂场景下，针对对称物体仍保持了鲁棒的几何对齐能力，证明了事件驱动视觉在极端条件下的有效性。
}

\newcommand{\mykeywords}        % 请在下面输入中文关键词，以中文分号分隔
{
    6D姿态估计；事件相机；多模态融合；AFDM；MTS
}

%---------------------------------------------------------------%
%  预置宏包和自定义命令(你可以补充需要的宏包和自定义命令)
%---------------------------------------------------------------%
\usepackage[pdfborder=0,CJKbookmarks=true]{hyperref}  % 使用内部超链接，其中第二个选项用于支持中文书签
\hypersetup{
    hidelinks=true,
}
%改变颜色

\addbibresource{example.bib}				% 填入参考文献库文件.bib

%===============================================================%
%  打印标题页信息，请不要修改这一部分
\begin{document}
\printtitlepage
%===============================================================%

%---------------------------------------------------------------%
%  正文内容从这里开始
%---------------------------------------------------------------%
\section{引言}
\subsection{研究背景与挑战}
随着机器人技术向太空探索和高速自动化领域延伸，视觉系统的工作环境变得日益严苛。例如，在太空碎片清理任务中，目标物体通常处于极端的明暗变化中（一面被太阳直射过曝，一面全黑）；在高速机械臂抓取任务中，rgb相机快速运动会导致成像模糊。

传统的6D姿态估计方法（如PoseCNN~\parencite{xiang2017posecnn}, PVNet~\parencite{peng2019pvnet}）主要依赖RGB图像的纹理特征，在上述场景下极易失效。虽然RGB-D方法（如DenseFusion~\parencite{wang2019densefusion}）引入了深度信息，但在物体边缘处，深度传感器常产生跳变噪声，且无法提供高频的运动信息。

\textbf{事件相机 (Event Camera)} 作为一种生物启发式传感器，具有微秒级的时间分辨率和极高的动态范围（HDR）。它仅记录亮度的变化，天然对运动边缘敏感，且不受运动模糊影响。这为解决极端场景下的姿态估计问题提供了新的契机。
\subsection{本文贡献}
为了充分利用事件相机的优势，本文提出了GMG-PVNet，主要贡献如下：
\begin{enumerate}
    \item \textbf{数据表示创新：MTS (Motion-Texture Surface)}。我们将时间维度的异步事件流编码为空间纹理，生成MTS图像。这不仅保留了高频运动信息，还解决了传统CNN无法直接处理异步数据的难题。
    \item \textbf{架构创新：AFDM模块}。针对3D事件点云（稀疏）与2D图像特征（密集）难以对齐的问题，我们提出了“注意力特征扩散模块”。它利用PointNet提取的全局几何特征作为“锚点”，通过注意力机制激活2D特征图的边缘响应。
    \item \textbf{融合机制：自适应门控融合}。设计了Gated Fusion模块，使网络能够根据输入质量（如RGB是否模糊），自适应地加权纹理特征与几何特征。
\end{enumerate}

\section{相关工作}

\subsection{6D物体姿态估计}
6D物体姿态估计旨在从图像中恢复物体的三维位置和旋转。基于RGB的方法通常通过回归2D关键点并利用PnP算法求解姿态，如PVNet~\parencite{peng2019pvnet}提出的像素级投票网络，显著提升了遮挡下的鲁棒性。基于RGB-D的方法如DenseFusion~\parencite{wang2019densefusion}，通过像素级融合RGB和深度特征，在纹理弱的物体上表现更好。然而，这些方法在高速运动或极端光照下，受限于传感器的物理特性，往往难以提取有效的特征。

\subsection{基于事件视觉的应用}
事件相机在光流估计、SLAM和边缘检测等领域已展现出巨大潜力。然而，将其应用于6D物体姿态估计的工作相对较少。现有的方法多是将事件流转换为体素网格（Voxel Grid）或时间面（Time Surface）作为CNN输入，忽略了事件点云本身包含的精细几何结构。

本文尝试结合MTS的纹理优势与原始点云的几何优势，实现更鲁棒的姿态估计。

\section{方法}

GMG-PVNet采用三流（Three-stream）并行架构，分别处理RGB图像、几何信息（Depth + MTS）以及原始事件点云。整体架构如图~\ref{fig:arch}所示。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/framework.png} % 请替换为你的架构图   
    \caption{GMG-PVNet 整体架构图。包含RGB分支、几何分支（MTS+Depth）和PointNet分支，通过Gated Fusion和AFDM模块进行多模态融合。}
	\label{fig:arch}
\end{figure} 

\subsection{多模态数据流}

\subsubsection{RGB Stream}
使用ResNet-18作为骨干网络，提取物体的颜色和语义纹理特征。这一分支在光照条件良好且物体静止时起主导作用。

\subsubsection{MTS Stream (Motion-Texture Surface)}
在处理事件相机的数据时，最常用的一种办法是将时空邻域内的事件通过简单方式（例如，通过像素级的事件计数或极性累加）转换为二维图像。事件帧的优势非常明显，主要因为：（i）它们是将陌生的事件流转换为熟悉二维表示的简单方法，该表示包含场景边缘的空间信息（这是自然图像中最具信息量的区域），（ii）它们不仅传递事件存在的信号，也传递事件缺失的信息（同样具有信息价值），（iii）其具有直观的解释（例如边缘图、亮度增量图像），以及（iv）它们是兼容传统计算机视觉的数据结构~\parencite{gallego2020event}。
但上述方法忽略了事件数据的高时间分辨率特性，导致图像被压缩，丢失了时间分辨率的信息。

因此，为了利用事件相机的高频特性，我们引入MTS表征。利用指数衰减函数，将一段时间 $\Delta t$ 内的事件流 $(x, y, t, p)$ 投影到2D平面。像素亮度 $I(x,y)$ 定义为：
\begin{equation}
    I(x, y) = \sum_{e_i \in \mathcal{E}(x,y)} p_i \cdot e^{-(t_{now} - t_i) / \tau}
\end{equation}

MTS相当于一个“时域高通滤波器”，自动过滤了静止背景，仅保留物体的运动轮廓。在光照剧烈变化或RGB模糊时，MTS仍能提供清晰的边缘纹理。同时替代传统的单色直方图。不仅解决了运动模糊问题，并为网络提供了显式的运动导数。MTS处理效果如图~\ref{fig:MTScomparison}所示。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/MTScomparison.png} % 请替换为你的架构图   
    \caption{MTS 处理效果对比图。（a）原始事件帧（b）MTS 图像（c）深度图（d）MTS+Depth 拼接图。}
	\label{fig:MTScomparison}
\end{figure} 


\subsubsection{Geometry Stream}
我们将深度图（Depth）与MTS在通道维度拼接，通过共享权重的ResNet提取物体的空间几何特征，如~\ref{fig:guided_6d_convincing_results}所示。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/guided_6d_convincing_results.png}    
    \caption{融合图。（a）MTS输入图像（b）深度图梯度（c）自适应空间域筛选（d）MTS边缘 （e）融合几何特征（f）深度与MTS融合表示。}
	\label{fig:guided_6d_convincing_results}
\end{figure} 

\subsection{AFDM：稀疏点云引导密集特征}
这是本文的核心创新。为了利用事件数据的微秒级精度，我们不仅使用了MTS图像，在这里还直接加入了原始事件点云。AFDM（Attentive Feature Diffusion Module）旨在解决稀疏点云与密集特征图的融合问题。

\begin{enumerate}
    \item \textbf{特征提取}：使用轻量级PointNet处理在线采样（Online Sampling）得到的 $N=1024$ 个事件点，提取包含精细时空信息的局部几何特征 $F_{point} \in \mathbb{R}^{N \times C}$。
    \item \textbf{空间散射 (Spatial Scattering)}：利用事件点的 $(x, y)$ 像素坐标，将PointNet提取的高维特征精准地“散射”回2D特征图的对应位置，形成稀疏特征图 $M_{sparse}$。
    \item \textbf{注意力扩散 (Attentive Diffusion)}：使用卷积层作为“扩散器”，将这些稀疏的几何特征向周围像素传播，从而“点亮”特征图中的物体边缘。
\end{enumerate}

\subsection{推理与几何修正}

\subsubsection{RANSAC Voting}
网络输出像素级向量场，表示每个像素指向物体关键点的单位向量。我们采用RANSAC算法聚类出9个2D关键点（8个角点+1个中心点），有效抵抗遮挡和离群点干扰。

\subsubsection{深度修正 (Depth-Guided Refinement)}
由于单目PnP对Z轴深度的估计存在尺度漂移，我们引入了几何约束策略。利用网络预测的Mask区域，统计深度图的中位数作为真实的Z轴距离。这是一种有效的传感器融合（Sensor Fusion）手段，显著提升了定位精度。

\section{实验}

\subsection{实验设置}
我们在YCB-Ev SD数据集上进行了评估。该数据集包含21类物体，是一个用于6DoF目标姿态估计的标准清晰度（SD）分辨率事件相机数据的合成数据集~\parencite{rojtberg2025ycb}。评价指标采用 \textbf{ADD-S}（针对对称/几何形状）和 \textbf{ADD}（针对非对称/纹理）。

\subsection{核心能力验证：单物体高精度实验}
为了验证模型在理想情况下的性能上限，我们选取了具有代表性的物体（如 \texttt{003\_cracker\_box}）进行独立训练与测试。


\subsubsection{训练演进过程评估 (Evaluation of Training Evolution)}

为了直观验证 GMG-PVNet 对多模态特征的整合能力，本文选取了训练过程中的两个关键阶段进行定性对比分析：训练初期（Epoch 5）与收敛阶段（Epoch 30）。具体对比结果如图 \ref{fig:training_evolution} 所示。

% --- 建议插入的对比图片结构 ---
\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\textwidth]{fig/epoch030.png} % 请替换为您的图片路径
    \caption{GMG-PVNet 训练过程可视化对比。第一行为 Epoch 5 结果，展示了初步的定位但边缘伴随噪声；第二行为 Epoch 30 结果，展示了平滑的掩码与高一致性的向量场。}
    \label{fig:training_evolution}
\end{figure}

\paragraph{1. 语义分割与目标定位的进化}

\textbf{训练初期 (Epoch 5)：} 可视化结果显示，模型在初期能够初步识别物体的空间位置，但生成的分割掩码（Mask）边缘存在明显的噪声，且对物体精细轮廓的拟合较差。这表明在训练开始阶段，模型正处于建立 RGB 纹理与 MTS 几何边缘关联的初步阶段，对于极端光照下模糊边缘的判别尚不够稳健。

\textbf{收敛阶段 (Epoch 30)：} 随着训练的推进，第 30 轮的可视化图表现出显著的改善。分割掩码变得连续且平滑，紧密贴合目标物体的物理边界。即使在背景复杂或光照剧烈变化的区域，模型也能通过 \textit{Gated Fusion} 模块有效抑制无效噪声，输出干净且准确的语义区域。

\paragraph{2. 向量场 (Vector Field) 的精确度提升}

在关键点预测方面，Epoch 30 的向量场方向表现出高度的收敛性。相比于 Epoch 5 中向量指向较为发散、甚至在物体中心区域出现方向冲突的情况，Epoch 30 的向量场在全图范围内展现出指向关键点的强一致性 (\textit{Consistency})。

这种提升直接得益于 \textit{AFDM (Attentive Feature Diffusion Module)} 的作用。随着训练的进行，模型学会了如何将点云提取的局部几何特征通过扩散机制补偿到图像特征图中，从而在局部细节处修正了由于图像模糊带来的向量预测偏移，显著增强了 RANSAC 投票过程的稳定性。


\subsection{单物体实验结果展示}


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.9\linewidth]{fig/singleobj.png} % 请替换为你的Loss曲线图
    \caption{单物体实验结果。GMG-PVNet在单物体场景下实现了超过90\%的ADD-S准确率，证明了事件数据在高精度姿态估计中的有效性。}
    \label{fig:epoch_030_vis}
\end{figure}

结果显示，在单物体实验中，GMG-PVNet实现了超过 \textbf{90\%} 的ADD-S准确率（$<10\%$直径误差）。这证明了在特征专注的情况下，引入事件和几何信息能极高精度地恢复物体的3D结构。MTS提供的强边缘特征使得关键点预测不再“缩成一团”，而是精准分布在物体角点。



\subsection{复杂场景分析：多物体实验}

为了全面评估模型在复杂场景下的泛化能力，我们在包含21类物体的YCB-Video数据集上进行了测试。实验评估了模型在不同严格程度下的准确率（Accuracy at threshold $d$）以及平均距离误差（Mean ADD Error）。详细的量化结果如表~\ref{tab:multi_obj_full}所示。

\begin{table}[htbp]
    \centering
    \caption{GMG-PVNet 在YCB-Video数据集上的多物体评估结果。其中 Acc($<k\%d$) 表示预测姿态误差小于物体直径 $k\%$ 的样本比例。}
    \label{tab:multi_obj_full}
    \resizebox{\linewidth}{!}{ % 自动缩放表格以适应页面宽度
    \begin{tabular}{l|l|c|ccc|c}
        \toprule
        \textbf{ID} & \textbf{Object Name} & \textbf{Count} & \textbf{Acc ($<10\%d$)} & \textbf{Acc ($<20\%d$)} & \textbf{Acc ($<50\%d$)} & \textbf{Mean Err (mm)} \\
        \midrule
        \multicolumn{7}{c}{\textit{Symmetric / Geometrically Distinct Objects}} \\
        \midrule
        13 & Bowl & 102 & \textbf{39.2\%} & \textbf{95.1\%} & 99.0\% & 20.4 \\
        20 & Extra Large Clamp & 101 & \textbf{59.4\%} & \textbf{88.1\%} & 100.0\% & 21.0 \\
        19 & Pitcher Base & 84 & 36.9\% & 73.2\% & 98.8\% & 20.7 \\
        10 & Banana & 82 & 39.0\% & 87.8\% & 98.8\% & 24.0 \\
        15 & Power Drill & 111 & 40.5\% & 88.3\% & 99.1\% & 23.5 \\
        \midrule
        \multicolumn{7}{c}{\textit{Texture-Dependent / Asymmetric Objects}} \\
        \midrule
        01 & Master Chef Can & 114 & 0.0\% & 47.4\% & 99.1\% & 22.5 \\
        02 & Cracker Box & 124 & 30.6\% & 79.8\% & 97.6\% & 29.2 \\
        05 & Mustard Bottle & 113 & 0.9\% & 76.1\% & \textbf{100.0\%} & 21.3 \\
        06 & Tuna Fish Can & 88 & 0.0\% & 65.9\% & 97.7\% & 19.8 \\
        21 & Foam Brick & 81 & 0.0\% & 82.7\% & 100.0\% & \textbf{17.4} \\
        \midrule
        \textbf{Avg} & \textbf{All Classes (Micro)} & \textbf{2000} & \textbf{11.8\%} & \textbf{67.0\%} & \textbf{97.4\%} & \textbf{22.7} \\
        \bottomrule
    \end{tabular}
    }
\end{table}

\textbf{结果分析与讨论：}

从表~\ref{tab:multi_obj_full}中可以观察到以下关键现象：

\begin{itemize}
    \item \textbf{极高的定位鲁棒性 (Localization Robustness)：} 
    在较为宽松的阈值下（Acc $<50\%d$），模型的平均准确率达到了 \textbf{97.4\%}。这意味着在绝大多数情况下，GMG-PVNet 能够成功检测到物体并将其定位在正确的空间范围内。平均位置误差仅为 \textbf{22.7mm}，这对于机械臂抓取等实际应用场景（通常容忍度在2-3cm）已具备极高的实用价值。
    
    \item \textbf{几何特征的主导作用：} 
    对于具有显著几何特征或对称性的物体（如 \textit{Extra Large Clamp, Bowl, Banana}），模型在严格阈值（$<10\%d$）下表现优异，最高达到了 59.4\%。这有力地证明了引入 \textbf{事件点云 (Point Cloud)} 和 \textbf{AFDM模块} 有效增强了网络对物体 3D 几何结构的感知能力，即使在纹理模糊的情况下也能通过几何边缘锁定姿态。
    
    \item \textbf{纹理歧义带来的挑战：} 
    对于纹理依赖型物体（如 \textit{Master Chef Can, Mustard Bottle}），虽然 Mean Error 很低（约2cm），但严格准确率接近 0\%。可视化分析表明，这是由于 \textbf{旋转歧义} 造成的。例如，圆柱体的罐头在旋转 180 度后几何形状不变，但纹理改变。由于 ResNet-18 的特征容量限制，模型在多物体混训时难以完美区分细微的纹理方向，导致预测姿态出现了翻转，从而被 ADD 指标判定为错误。然而，从几何占位（IoU）的角度来看，预测框依然完美贴合了物体。
\end{itemize}

\textbf{结果解读：}
\begin{itemize}
    \item \textbf{几何定位强：} 在对称物体上，ADD-S分数较高，说明模型成功捕捉到了物体的几何形状和空间位置。
    \item \textbf{纹理歧义：} 在非对称物体上，ADD分数较低。可视化分析发现，这是由于ResNet-18容量限制及纹理特征被弱化，导致模型出现了180°的旋转歧义（即头尾颠倒），但包围盒的重合度依然很高。
\end{itemize}

\subsection{消融实验}
为了验证多模态融合的必要性，我们对比了“纯RGB”，“RGB加深度与MTS融合信息”与“多模态融合（Ours）”的效果。

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.8\linewidth]{fig/ablation experiment.png} % 请替换为你的Loss曲线图
    \caption{消融实验对比。引入事件点云后，模型的平均几何误差显著降低。}
	\label{fig:ablation}
\end{figure} 

对比结果表明，引入MTS和事件点云后，模型的平均几何误差（Mean ADD Error）显著降低。在定性分析中，当RGB图像因光照较暗导致纹理不清时，纯RGB模型往往无法检测到物体；而GMG-PVNet凭借MTS提供的清晰轮廓，依然能准确框出物体位置。

\subsection{定性结果展示}
图~\ref{fig:vis}展示了模型在复杂场景下的推理结果。绿色框为模型预测的3D包围盒。可以看出，即使在存在遮挡和背景杂波的情况下，模型依然能够准确地定位物体。

\begin{figure}[htbp]
    \centering
    % \includegraphics[width=1.0\linewidth]{fig/vis_result.jpg} % 请替换为你的可视化结果图
    \caption{GMG-PVNet 在YCB-Video数据集上的定性结果。绿色框为预测结果，蓝色框为真值。}
	\label{fig:vis}
\end{figure} 

\section{总结与未来展望}

本文针对极端光照和高速运动场景，提出了一种基于事件相机的6D姿态估计网络GMG-PVNet。通过MTS表征和AFDM模块，我们成功地将高频事件信息融合进姿态估计流程。

\textbf{主要结论：} (1) 事件数据有效性：MTS和事件点云显著增强了模型对物体边缘的感知能力，特别是在几何定位精度上表现优异。(2) 鲁棒性：通过深度修正和动态掩码策略，模型在复杂背景下展现了极强的抗干扰能力。

\textbf{局限与展望：} 目前模型在多物体分类和纹理方向识别上仍有提升空间。未来工作将集中在引入更大容量的Backbone以解决纹理混淆问题，并加入基于RGB的Refinement网络以消除旋转歧义。


%---------------------------------------------------------------%
%  参考文献
%---------------------------------------------------------------%
\printbibliography

%===============================================================%

\end{document}
